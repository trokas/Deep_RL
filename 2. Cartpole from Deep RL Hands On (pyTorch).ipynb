{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balanding Cart Pole\n",
    "\n",
    "Alternative implementation of Cart Pole balancing algorithm using pyTorch. Main ideas come from a great book Deep Reinforcement Learning Hands-On by Maxim Laptan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 20    # number of episodes to complete before agent update\n",
    "PERCENTILE = 0.8   # how many episodes to train from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trokas/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 36.400000000000006\n",
      "40 22.800000000000008\n",
      "60 27.200000000000003\n",
      "80 37.2\n",
      "100 44.400000000000006\n",
      "120 55.20000000000001\n",
      "140 31.400000000000006\n",
      "160 48.00000000000001\n",
      "180 38.40000000000002\n",
      "200 48.400000000000006\n",
      "220 89.60000000000002\n",
      "240 89.4\n",
      "260 95.4\n",
      "280 130.6\n",
      "300 93.40000000000003\n",
      "320 86.20000000000002\n",
      "340 95.2\n",
      "360 117.4\n",
      "380 133.8\n",
      "400 117.2\n",
      "420 145.60000000000002\n",
      "440 148.0\n",
      "460 174.60000000000002\n",
      "480 136.0\n",
      "500 149.4\n",
      "520 176.6\n",
      "540 175.0\n",
      "560 179.20000000000005\n",
      "580 200.0\n",
      "WIN!!!\n"
     ]
    }
   ],
   "source": [
    "agent = nn.Sequential(\n",
    "            nn.Linear(4, HIDDEN_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_SIZE, 2)\n",
    "        )\n",
    "sm = nn.Softmax()\n",
    "\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=agent.parameters(), lr=0.01)\n",
    "\n",
    "episodes = []\n",
    "state_history = []\n",
    "action_history = []\n",
    "episode_reward = 0.0\n",
    "batch_episode_count = 0\n",
    "state = env.reset()\n",
    "\n",
    "n_episodes = 0\n",
    "\n",
    "while True:\n",
    "    action_prob = sm(agent.forward(torch.FloatTensor(state)))   \n",
    "    action = np.random.choice([0, 1], p=action_prob.detach().numpy())   \n",
    "    state_history.append(state)\n",
    "    \n",
    "    action_history.append(action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        n_episodes += 1\n",
    "        episodes.append([episode_reward,\n",
    "                         np.array(state_history),\n",
    "                         np.array(action_history)])\n",
    "        # repeat for BATCH_SIZE episodes\n",
    "        if len(episodes) == BATCH_SIZE:\n",
    "            # Take best state action pairs based on given percentile           \n",
    "            df = pd.DataFrame(episodes, columns=['reward', 'state', 'action'])\n",
    "            p = df['reward'].quantile(PERCENTILE)\n",
    "            df = df[df['reward'] > p]\n",
    "            # Print some stats\n",
    "            print(n_episodes, p)\n",
    "            if p == 200.0:\n",
    "                print('WIN!!!')\n",
    "                break\n",
    "            # Run optimizer\n",
    "            optimizer.zero_grad()\n",
    "            action_scores = agent.forward(torch.FloatTensor(\n",
    "                np.concatenate(df['state'].values)))\n",
    "            taken_actions = torch.LongTensor(\n",
    "                np.concatenate(df['action'].values))\n",
    "            loss = objective(action_scores, taken_actions)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # reset batch\n",
    "            episodes = []\n",
    "\n",
    "        # reset episode holders\n",
    "        episode_reward = 0.0\n",
    "        state_history = []\n",
    "        action_history = []\n",
    "        state = env.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
