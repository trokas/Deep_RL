{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning in Tensorflow Part 2: Vanilla Policy Gradient Agent\n",
    "\n",
    "This tutorial contains a simple example of how to build a policy-gradient based agent that can solve the CartPole problem with pyTorch. For original series with tensorflow code samples see [Medium post](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.mtwpvfi8b).\n",
    "\n",
    "Also take a look at alternative implementation of Cart Pole policy based on book Deep Reinforcement Learning Hands-On in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xrange = range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Policy-Based Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Take 1D float array of rewards and compute discounted reward .\n",
    "    For example: 0, 1, 2, 3, 4 will be trasnformed to 9.7, 9.8, 8.9, 6.96, 4\n",
    "    \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trokas/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 23.71\n",
      "200 25.7\n",
      "300 24.1\n",
      "400 24.53\n",
      "500 27.45\n",
      "600 26.23\n",
      "700 32.69\n",
      "800 31.85\n",
      "900 43.0\n",
      "1000 41.57\n",
      "1100 50.14\n",
      "1200 56.34\n",
      "1300 60.61\n",
      "1400 60.38\n",
      "1500 77.05\n",
      "1600 97.14\n",
      "1700 118.26\n",
      "1800 121.57\n",
      "1900 142.01\n",
      "2000 136.88\n",
      "2100 151.31\n",
      "2200 174.71\n",
      "2300 175.93\n",
      "2400 167.19\n",
      "2500 176.27\n",
      "2600 185.49\n",
      "2700 182.37\n",
      "2800 181.99\n",
      "2900 187.86\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 3000 # Set total number of episodes to train agent on.\n",
    "max_ep = 999\n",
    "\n",
    "agent = torch.nn.Sequential(\n",
    "          torch.nn.Linear(4, 8),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(8, 2),\n",
    "          torch.nn.Softmax())\n",
    "adam = torch.optim.Adam(agent.parameters(), lr=1e-3)\n",
    "\n",
    "i = 0\n",
    "total_reward = []\n",
    "\n",
    "for i in range(total_episodes):\n",
    "    state = env.reset()\n",
    "    running_reward = 0\n",
    "    state_history = []\n",
    "    action_history = []\n",
    "    reward_history = []\n",
    "    for j in range(max_ep):\n",
    "        # Probabilistically pick an action given our network outputs\n",
    "        action_prob = agent.forward(torch.tensor(state, dtype=torch.float))\n",
    "        action = np.random.choice([0, 1], p=action_prob.detach().numpy())\n",
    "\n",
    "        state_history.append(state)\n",
    "        action_history.append(action)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        reward_history.append(reward)\n",
    "        running_reward += reward\n",
    "\n",
    "        if done:\n",
    "            discounted_rewards = torch.tensor(discount_rewards(np.array(reward_history)), dtype=torch.float)\n",
    "            action_prob = agent.forward(torch.tensor(np.stack(state_history), dtype=torch.float))\n",
    "            loss = -torch.mean(torch.log(action_prob[np.arange(j+1), action_history]) * discounted_rewards)\n",
    "\n",
    "            adam.zero_grad()\n",
    "            loss.backward()\n",
    "            adam.step()\n",
    "\n",
    "            total_reward.append(running_reward)\n",
    "            break\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(i, np.mean(total_reward[-100:]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
