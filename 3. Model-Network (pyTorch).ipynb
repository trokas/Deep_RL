{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning with Tensorflow: Part 3 - Model-Based RL\n",
    "\n",
    "In this iPython notebook we implement a policy and model network which work in tandem to solve the CartPole reinforcement learning problem using pyTorch. Original series with tensorflow implementation can be found on [Medium](https://medium.com/p/9a6fe0cce99)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xrange = range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(4, 8)\n",
    "        self.fc2 = torch.nn.Linear(8, 2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigm = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigm(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ModelNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Input: (state, action) [5 x 1]\n",
    "    Output: (next state) [4 x 1]\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ModelNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(5, 256)\n",
    "        self.fc2 = torch.nn.Linear(256, 256)\n",
    "        self.fc_state = torch.nn.Linear(256, 4)\n",
    "        self.fc_reward = torch.nn.Linear(256, 1)\n",
    "        self.fc_done = torch.nn.Linear(256, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        state = self.fc_state(out)\n",
    "        return state\n",
    "\n",
    "\n",
    "class create_model_env():\n",
    "    \"\"\"\n",
    "    Simulated enviroment based on model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, state=None):\n",
    "        self.model = model\n",
    "        self.state = self.reset() if state is None else state\n",
    "        self.n = 0\n",
    "    def update(self, model):\n",
    "        self.model = model\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Predicts next state based on current state and action taken.\n",
    "        \n",
    "        Returns done status if:\n",
    "            * Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)\n",
    "            * Pole Angle is more than ±12°\n",
    "            * Episode length is greater than 200\n",
    "        \"\"\"\n",
    "        state = model.forward(torch.tensor(np.hstack([self.state, action]), dtype=torch.float))\n",
    "        state = state.detach().numpy()\n",
    "        self.n += 1\n",
    "        self.state = state\n",
    "        # Episode termination\n",
    "        done = (abs(state[0]) > 2.4) or (abs(state[2]) > 12 * 2 * np.math.pi / 360) or (self.n == 200)\n",
    "        return state, 1.0, done, {}\n",
    "    def reset(self):\n",
    "        # Generate reasonable starting point\n",
    "        self.state = np.random.uniform(-0.05, 0.05, [4])\n",
    "        self.n = 0\n",
    "        return self.state\n",
    "\n",
    "\n",
    "def discount_rewards(r, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Take 1D float array of rewards and compute discounted reward .\n",
    "    For example: 0, 1, 2, 3, 4 will be trasnformed to 9.7, 9.8, 8.9, 6.96, 4\n",
    "    \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "# Initiate networks\n",
    "policy_loss = []\n",
    "model_loss = []\n",
    "model = ModelNet()\n",
    "model_adam = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "policy = PolicyNet()\n",
    "policy_adam = torch.optim.Adam(policy.parameters(), lr=1e-3)\n",
    "\n",
    "real_env_episodes = 0\n",
    "model_env_episodes = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trokas/anaconda3/envs/rl/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 134.0\n",
      "100 167.76\n",
      "200 190.85\n",
      "300 190.06\n",
      "400 194.86\n",
      "500 189.66\n",
      "600 195.81\n",
      "700 196.81\n",
      "800 196.14\n",
      "900 199.0\n",
      "Real world episodes: 3000, Model episodes: 5000\n"
     ]
    }
   ],
   "source": [
    "drawFromModel = True     # When set to True, will use model for observations\n",
    "trainTheModel = False     # Whether to train the model\n",
    "trainThePolicy = True    # Whether to train the policy\n",
    "\n",
    "total_episodes = 1000 # Set total number of episodes to train agent on.\n",
    "max_ep = 1000\n",
    "\n",
    "total_reward = []\n",
    "\n",
    "# Initiate enviroment\n",
    "env = create_model_env(model) if drawFromModel else gym.make('CartPole-v0')\n",
    "\n",
    "for i in range(total_episodes):\n",
    "    if drawFromModel:\n",
    "        model_env_episodes += 1\n",
    "    else:\n",
    "        real_env_episodes += 1\n",
    "    \n",
    "    state = env.reset()\n",
    "    running_reward = 0\n",
    "    state_history = []\n",
    "    action_history = []\n",
    "    reward_history = []\n",
    "\n",
    "    for j in range(max_ep):\n",
    "        if j == 999:\n",
    "            print('Max episodes reached')\n",
    "\n",
    "        action_prob = policy.forward(torch.tensor(state, dtype=torch.float))\n",
    "        action = np.random.choice([0, 1], p=action_prob.detach().numpy())\n",
    "\n",
    "        state_history.append(state)\n",
    "        action_history.append(action)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        reward_history.append(reward)\n",
    "        running_reward += reward\n",
    "\n",
    "        if done:\n",
    "            total_reward.append(running_reward)\n",
    "\n",
    "            if trainThePolicy:\n",
    "\n",
    "                discounted_rewards = torch.tensor(discount_rewards(np.array(reward_history)), dtype=torch.float)\n",
    "                action_prob = policy.forward(torch.tensor(np.stack(state_history), dtype=torch.float))\n",
    "                loss = -torch.mean(torch.log(action_prob[np.arange(j+1), action_history]) * discounted_rewards)\n",
    "                policy_loss.append(loss)\n",
    "\n",
    "                policy_adam.zero_grad()\n",
    "                loss.backward()\n",
    "                policy_adam.step()\n",
    "\n",
    "            if trainTheModel:\n",
    "\n",
    "                in_state_action = np.hstack([np.array(state_history), np.array(action_history)[:, np.newaxis]])[:-1]\n",
    "                out_state = model.forward(torch.tensor(in_state_action, dtype=torch.float))\n",
    "                exp_state = torch.tensor(np.array(state_history)[1:], dtype=torch.float)\n",
    "\n",
    "                loss = torch.mean((exp_state - out_state)**2)\n",
    "                model_loss.append(loss)\n",
    "\n",
    "                model_adam.zero_grad()\n",
    "                loss.backward()\n",
    "                model_adam.step()\n",
    "\n",
    "            env.reset()\n",
    "            break\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(i, np.mean(total_reward[-100:]))\n",
    "\n",
    "print('Real world episodes: {0}, Model episodes: {1}'.format(real_env_episodes, model_env_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate environment model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f61686b2748>]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark:** with 3000 real world episodes Vanilla-Policy manages to reach score ~180 out of last 100 episodes.\n",
    "\n",
    "**Result:** with 3000 real world episodes and 5000 model interactions we have much more robust model which manages to win 100 last games with score 200! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
